{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network API\n",
    "\n",
    "### Features:\n",
    "- Vectorized\n",
    "- Customizable structure of NN:\n",
    "    - number of layers\n",
    "    - number of units in each layer\n",
    "    - activation function of each layer\n",
    "- Choice of activation functions:\n",
    "    - ReLU\n",
    "    - tanh\n",
    "    - sigmoid\n",
    "    - Leaky ReLU (WIP)\n",
    "    - Softmax (WIP)\n",
    "- Choice of optimization functions:\n",
    "    - Gradient Descent\n",
    "    - GD with momentum\n",
    "    - GD with RMSProp\n",
    "    - Adam\n",
    "- Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of Notebook:\n",
    "\n",
    "1. **Network Structure definition**\n",
    "2. **Forward Propagation**\n",
    "3. **Cost computation**\n",
    "4. **Backward Propagation**\n",
    "5. **Optimization Algorithms**\n",
    "    1. Gradient Descent\n",
    "    2. Mini-Batch Gradient Descent\n",
    "    3. Momentum\n",
    "    4. RMSProp\n",
    "    5. Adam\n",
    "5. **Gradient Checking**\n",
    "5. **Model Training function**\n",
    "6. **Prediction implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: xdevapps (use `wandb login --relogin` to force relogin)\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ADMIN/.netrc\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nn_activation_funcs import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login(key='c5df0c7d963898d355d37b974f08c0a4ff75abef')\n",
    "\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the structure of the Neural Network\n",
    "\n",
    "1. Define the size of each layer in `layer_dims`\n",
    "    - layer 0 = Input layer\n",
    "    - layer 1 = Hidden layer\n",
    "    - ...\n",
    "    - layer N-1 = Hidden layer\n",
    "    - layer N = Output layer\n",
    "    \n",
    "2. Define the activation of each layer in `layer_activations`\n",
    "    - `'r'` = ReLU\n",
    "    - `'t'` = tanh\n",
    "    - `'s'` = sigmoid\n",
    "    \n",
    "**NOTE:** Length of `layer_activations` is 1 less than `layer_dims`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example -\n",
    "- layer_dims = [4, 3, 5, 3, 1]   (len = 5)\n",
    "- layer_activations = ['r', 'r', 'r', 's']   (len = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(21)\n",
    "    parameters = {'W': [None], 'b': [None]}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W'].append(np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01)\n",
    "        parameters['b'].append(np.zeros((layer_dims[l], 1)))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Propagation\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]} $$\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_layer(A_prev, W, b, activation):\n",
    "    # Linear step\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    # Activation step\n",
    "    A = None\n",
    "    if activation == 'r':\n",
    "        A = relu_fwd(Z)\n",
    "    elif activation == 't':\n",
    "        A = tanh_fwd(Z)\n",
    "    elif activation == 's':\n",
    "        A = sigmoid_fwd(Z)\n",
    "    \n",
    "    cache = (A_prev, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(layer_dims, layer_activations, X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        # Get parameters for current layer\n",
    "        A_prev = A\n",
    "        W = parameters['W'][l]\n",
    "        b = parameters['b'][l]\n",
    "        activation = layer_activations[l]\n",
    "        \n",
    "        # Compute Activations and caches\n",
    "        A, cache = forward_propagation_layer(A_prev, W, b, activation)\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost Function\n",
    "\n",
    "We will be using cross-entropy loss function\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} log\\left(a^{[L](i)}\\right) + \\left(1 - y^{(i)}\\right) log\\left(1 - a^{[L](i)}\\right) \\right) $$\n",
    "\n",
    "In a vectorized implementation this becomes -\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\left( Y \\cdot log\\left(A^{[L]T}\\right) + \\left(1 - Y\\right) \\cdot log\\left(1 - A^{[L]T}\\right) \\right) $$\n",
    "\n",
    "where,\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> Vector </th>\n",
    "        <th> Shape </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $Y$ </td>\n",
    "        <td> $(1, m)$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $A^{[L]T}$ </td>\n",
    "        <td> $(m$$,$ $1)$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $J$ </td>\n",
    "        <td> $(1, 1)$ </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, AL):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = - (np.dot(Y, np.log(AL.T)) + np.dot(1 - Y, np.log(1 - AL.T))) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backward Propagation\n",
    "\n",
    "$$ dZ^{[l]} = dA^{[l]} * g'^{[l]}(Z^{[l]}) $$\n",
    "$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1]T} $$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)} $$\n",
    "$$ dA^{[l-1]} = W^{[l]T} dZ^{[l]} $$\n",
    "\n",
    "The derivative for the last layer is given by:\n",
    "\n",
    "$$ dA^{[L]} = - \\frac{Y}{A^{[L]}} + \\frac{1-Y}{1-A^{[L]}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_layer(dA, Z, A_prev, A, W, b, activation):\n",
    "    # Calculate dZ\n",
    "    dZ = None\n",
    "    if activation == 'r':\n",
    "        dZ = dA * relu_bwd(Z, A)\n",
    "    elif activation == 't':\n",
    "        dZ = dA * tanh_bwd(Z, A)\n",
    "    elif activation == 's':\n",
    "        dZ = dA * sigmoid_bwd(Z, A)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # Calculate dW, db, dA_prev\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(layer_dims, layer_activations, A, Y, caches, parameters):\n",
    "    L = len(layer_dims)\n",
    "    gradients = {\n",
    "        #'dA': [None for _ in range(L)],\n",
    "        'dW': [None for _ in range(L)],\n",
    "        'db': [None for _ in range(L)],\n",
    "    }\n",
    "    m = A.shape[1]\n",
    "    Y = Y.reshape(A.shape)\n",
    "    \n",
    "    # Derivative of last layer \n",
    "    dA = - np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "    #gradients['dA'][L] = dA\n",
    "    \n",
    "    A_cur = A\n",
    "    \n",
    "    # For subsequent layers \n",
    "    for l in reversed(range(1, L)): # from L to 1, where A[0] is input layer\n",
    "        # Get parameters for current layer\n",
    "        # Note: l corresponds to previous layer's dA, so we deal with (l+1)th layer's parameters\n",
    "        A_prev, Z = caches[l-1]\n",
    "        W = parameters['W'][l]\n",
    "        b = parameters['b'][l]\n",
    "        activation = layer_activations[l]\n",
    "        \n",
    "        dA_prev, dW, db = backward_propagation_layer(dA, Z, A_prev, A_cur, W, b, activation)\n",
    "        \n",
    "        # Store gradients for use in optimization\n",
    "        #gradients['dA'][l] = dA_prev\n",
    "        gradients['dW'][l] = dW\n",
    "        gradients['db'][l] = db\n",
    "        \n",
    "        # Replace current A with previous layer's A for next loop\n",
    "        A_cur = A_prev\n",
    "        dA = dA_prev\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "A simple optimization algorithm where we shift the weights towards lower gradients in order to find a minimum\n",
    "\n",
    "$$ W := W - \\alpha dW $$\n",
    "$$ b := b - \\alpha db $$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter called **Learning Rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_update(layer_dims, parameters, gradients, learning_rate):\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W'][l] -= learning_rate * gradients['dW'][l]\n",
    "        parameters['b'][l] -= learning_rate * gradients['db'][l]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batches\n",
    "\n",
    "In simple gradient descent (aka batch gradient descent), we use the entire training set for every single parameter update. This can take a very long time for a single gradient descent step when the data set is huge. To improve the learning performance, we split up the training set into **mini-batches** and update the parameters on each mini-batch.\n",
    "\n",
    "This has the advantage that updates are faster as only a part of the data set has to be processed.  \n",
    "But when using mini-batch GD, the cost function oscillates towards the minimum instead of steadily decreasing, which can make checking the progress of the model difficult.\n",
    "\n",
    "To implement Mini-batches in our optimization we need to implement the following:\n",
    "- **Shuffle** : Creating a shuffled version of the training set will help to make each mini-batch have a similar probability distribution\n",
    "- **Partition** : Partition the shuffled dataset into mini-batches of size `mini_batch_size`\n",
    "- Then while calling the `forward_propagation` and `back_propagation` functions pass in the mini-batches one by one in place of `X` and `Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    if mini_batch_size == 0:\n",
    "        return [(X, Y)]\n",
    "        \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "    \n",
    "    # Partition the shuffled set\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    return mini_batches    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum\n",
    "\n",
    "To reduce the oscillations induced in the optimization while using gradient descent, we use **momentum** to drive the weights towards the minimum due to accumulated velocity.  \n",
    "Momentum is calculated using *Exponential Weighted Average*\n",
    "\n",
    "$$ V_{dW} = \\beta V_{dW} + (1 - \\beta) dW $$\n",
    "$$ V_{db} = \\beta V_{db} + (1 - \\beta) db $$\n",
    "\n",
    "$$ W := W - \\alpha V_{dW} $$\n",
    "$$ b := b - \\alpha V_{db} $$\n",
    "\n",
    "where $\\alpha$ is Learning Rate, and $\\beta$ is another hyperparameter. (If in doubt take $\\beta = 0.9$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_momentum(layer_dims):\n",
    "    V = {'dW': [None], 'db': [None]}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        V['dW'].append(np.zeros((layer_dims[l], layer_dims[l-1])))\n",
    "        V['db'].append(np.zeros((layer_dims[l], 1)))\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_momentum_update(layer_dims, parameters, gradients, V, beta, learning_rate):\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        V['dW'][l] = beta * V['dW'][l] + (1 - beta) * gradients['dW'][l]\n",
    "        V['db'][l] = beta * V['db'][l] + (1 - beta) * gradients['db'][l]\n",
    "        \n",
    "        parameters['W'][l] -= learning_rate * V['dW'][l]\n",
    "        parameters['b'][l] -= learning_rate * V['db'][l]\n",
    "        \n",
    "    return parameters, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with RMSProp\n",
    "\n",
    "Another method to reduce oscillations caused by large uneven gradients in gradient descent, is to normalize the gradients with the **Root Mean Square** of the gradients. This was introduced by Geoff Hinton in his Coursera Tutorial.\n",
    "It works by scaling up smaller gradients and scaling down larger gradients which can improve learning.\n",
    "\n",
    "$$ S_{dW} = \\beta S_{dW} + (1 - \\beta) dW^2 $$\n",
    "$$ S_{db} = \\beta S_{db} + (1 - \\beta) db^2 $$\n",
    "\n",
    "$$ W := W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}} $$\n",
    "$$ b := b - \\alpha \\frac{db}{\\sqrt{S_{db}}} $$\n",
    "\n",
    "where $\\alpha$ is Learning Rate, and $\\beta$ is another hyperparameter. (If in doubt take $\\beta = 0.9$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_RMSprop(layer_dims):\n",
    "    S = {'dW': [None], 'db': [None]}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        S['dW'].append(np.zeros((layer_dims[l], layer_dims[l-1])))\n",
    "        S['db'].append(np.zeros((layer_dims[l], 1)))\n",
    "        \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSprop_update(layer_dims, parameters, gradients, S, beta, learning_rate):\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        S['dW'][l] = beta * S['dW'][l] + (1 - beta) * np.square(gradients['dW'][l])\n",
    "        S['db'][l] = beta * S['db'][l] + (1 - beta) * np.square(gradients['db'][l])\n",
    "        \n",
    "        parameters['W'][l] -= learning_rate * parameters['dW'][l] / np.sqrt(S['dW'][l])\n",
    "        parameters['b'][l] -= learning_rate * parameters['db'][l] / np.sqrt(S['db'][l])\n",
    "        \n",
    "    return parameters, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization\n",
    "\n",
    "Proposed by Kingma & Ba in 2015, Adam combines the ideas of momentum and RMSprop to achieve **Adaptive Momentum Estimation** (acronymed as Adam)\n",
    "\n",
    "$$ V_{dW} = \\beta_1 V_{dW} + (1 - \\beta_1) dW $$\n",
    "$$ V_{db} = \\beta_1 V_{db} + (1 - \\beta_1) db $$\n",
    "\n",
    "$$ S_{dW} = \\beta_2 S_{dW} + (1 - \\beta_2) dW^2 $$\n",
    "$$ S_{db} = \\beta_2 S_{db} + (1 - \\beta_2) db^2 $$\n",
    "\n",
    "for iteration $t$ -\n",
    "$$ V_{dW}^{corrected} = \\frac{V_{dW}}{1 - \\beta_1^t} , V_{db}^{corrected} = \\frac{V_{db}}{1 - \\beta_1^t} $$\n",
    "$$ S_{dW}^{corrected} = \\frac{S_{dW}}{1 - \\beta_2^t} , S_{db}^{corrected} = \\frac{S_{db}}{1 - \\beta_2^t} $$\n",
    "\n",
    "$$ W := W - \\alpha \\frac{V_{dW}^{corrected}}{\\sqrt{S_{dW}^{corrected} + \\epsilon}} $$\n",
    "$$ b := b - \\alpha \\frac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected} + \\epsilon}} $$\n",
    "\n",
    "where $\\alpha$ is Learning Rate, $\\beta_1$ and $\\beta_2$ are hyperparameters, and $\\epsilon$ is a very small value to avoid division by zero.\n",
    "The suggested values for the hyperparameters are: \n",
    "<table>\n",
    "    <tr>\n",
    "        <td> $\\beta_1$ </td>\n",
    "        <td> $0.9$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $\\beta_2$ </td>\n",
    "        <td> $0.999$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $\\epsilon$ </td>\n",
    "        <td> $10^{-8}$ </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Adam(layer_dims):\n",
    "    V = {'dW': [None], 'db': [None]}\n",
    "    S = {'dW': [None], 'db': [None]}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        V['dW'].append(np.zeros((layer_dims[l], layer_dims[l-1])))\n",
    "        V['db'].append(np.zeros((layer_dims[l], 1)))\n",
    "        \n",
    "        S['dW'].append(np.zeros((layer_dims[l], layer_dims[l-1])))\n",
    "        S['db'].append(np.zeros((layer_dims[l], 1)))\n",
    "        \n",
    "    return V, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_update(layer_dims, parameters, gradients, V, S, t,  learning_rate,\n",
    "                beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        V['dW'][l] = beta1 * V['dW'][l] + (1 - beta1) * gradients['dW'][l]\n",
    "        V['db'][l] = beta1 * V['db'][l] + (1 - beta1) * gradients['db'][l]\n",
    "        \n",
    "        S['dW'][l] = beta2 * S['dW'][l] + (1 - beta2) * np.square(gradients['dW'][l])\n",
    "        S['db'][l] = beta2 * S['db'][l] + (1 - beta2) * np.square(gradients['db'][l])\n",
    "        \n",
    "        V_dW_corrected = V['dW'][l] / (1 - beta1**t)\n",
    "        V_db_corrected = V['db'][l] / (1 - beta1**t)\n",
    "        \n",
    "        S_dW_corrected = S['dW'][l] / (1 - beta2**t)\n",
    "        S_db_corrected = S['db'][l] / (1 - beta2**t)\n",
    "        \n",
    "        parameters['W'][l] -= learning_rate * V_dW_corrected / np.sqrt(S_dW_corrected + epsilon)\n",
    "        parameters['b'][l] -= learning_rate * V_db_corrected / np.sqrt(S_db_corrected + epsilon)\n",
    "        \n",
    "    return parameters, V, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Checking\n",
    "\n",
    "Before we go ahead and train our model, let's verify that our implementation of back propagation is correct.\n",
    "\n",
    "In implementation of a neural network, the area for the most probability of error is in back propagation. It can be quite complex due to the many gradients/derivatives and thus it is always better to check that we have performed the computations correctly.\n",
    "\n",
    "#### How to check gradients?\n",
    "\n",
    "We can approximate gradients using definition of derivative --\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\rightarrow 0} \\frac{J(\\theta+ \\varepsilon) - J(\\theta - \\varepsilon)}{2\\varepsilon} $$\n",
    "\n",
    "where, $\\theta$ is a parameter and $J$ is the cost function.\n",
    "\n",
    "Then we find the relative difference between `gradapprox` (the approximate) and `gradient` (the result of the back propagation step) using the 2-norm --\n",
    "\n",
    "$$ \\textit{difference} = \\frac{\\mid\\mid\\textit{gradapprox} - \\textit{gradient}\\mid\\mid}{\\mid\\mid\\textit{gradapprox}\\mid\\mid_2 + \\mid\\mid\\textit{gradient}\\mid\\mid_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps to compute gradient check:\n",
    "Flatten all parameters $W_i$ and $b_i$ and concatenate them into one long vector as \n",
    "$$ \\theta = (W^{1,1}_1, W^{1,2}_1, ... , W^{n^1,n^{0}-1}_1, W^{n^1,n^{0}}_1, b^{1}_1, ..., b^{n^1}_1, ... ) $$ \n",
    "\n",
    "For each $i$ in _parameters_ -\n",
    "1. Copy $\\theta$ to $\\theta^+_i$ and $\\theta^-_i$\n",
    "1. Compute $\\theta^+_i = \\theta_i + \\varepsilon$ and $\\theta^-_i = \\theta_i - \\varepsilon$\n",
    "    * All parameters except the $i^{th}$ remain unchanged\n",
    "2. Use forward propagation to compute $\\hat{Y}^+_i$ and $\\hat{Y}^-_i$\n",
    "3. Compute costs $J^+_i = J(\\hat{Y}^+_i)$ and $J^-_i = J(\\hat{Y}^-_i)$\n",
    "4. Compute $\\textit{gradapprox}_i = \\frac{J^+ - J^-}{2\\varepsilon}$\n",
    "\n",
    "Finally,  \n",
    "Calculate $\\textit{difference}$ using `np.linalg.norm()`\n",
    "\n",
    "###### NOTE: Use gradient checking before updating parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First to flatten all the parameters -\n",
    "def flatten_params(layer_dims, parameters):\n",
    "    theta = None\n",
    "    for l in range(1, len(layer_dims)):    \n",
    "        for key in parameters:\n",
    "            flattened = np.reshape(parameters[key][l], (-1, 1))\n",
    "            if theta is None:\n",
    "                theta = flattened\n",
    "            else:\n",
    "                theta = np.concatenate((theta, flattened), axis = 0)\n",
    "    return theta\n",
    "\n",
    "# Then to inflate theta back into parameter dictionary -\n",
    "def inflate_params(layer_dims, theta):\n",
    "    params = {'W': [None], 'b': [None]}\n",
    "    L = len(layer_dims)\n",
    "    count = 0\n",
    "    for l in range(1, L):\n",
    "        param_W = np.copy(theta[count : count + layer_dims[l] * layer_dims[l-1]]).reshape((layer_dims[l], layer_dims[l-1]))\n",
    "        params['W'].append(param_W)\n",
    "        \n",
    "        count += layer_dims[l] * layer_dims[l-1]\n",
    "        \n",
    "        param_b = np.copy(theta[count : count + layer_dims[l]]).reshape((layer_dims[l], 1))\n",
    "        params['b'].append(param_b)\n",
    "        \n",
    "        count += layer_dims[l]\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(X, Y, layer_dims, layer_activations, parameters, gradients, epsilon = 1e-7):\n",
    "    theta = flatten_params(layer_dims, parameters)\n",
    "    num_params = theta.shape[0]\n",
    "    gradapprox = np.zeros((num_params, 1))\n",
    "    gradient = flatten_params(layer_dims, gradients)\n",
    "    \n",
    "    for i in range(num_params):\n",
    "        theta_plus = np.copy(theta)\n",
    "        theta_plus[i][0] += epsilon\n",
    "        Y_plus, _ = forward_propagation(layer_dims, layer_activations, X, inflate_params(layer_dims, theta_plus))\n",
    "        J_plus = compute_cost(Y, Y_plus)\n",
    "        \n",
    "        theta_minus = np.copy(theta)\n",
    "        theta_minus[i][0] -= epsilon\n",
    "        Y_minus, _ = forward_propagation(layer_dims, layer_activations, X, inflate_params(layer_dims, theta_minus))\n",
    "        J_minus = compute_cost(Y, Y_minus)\n",
    "        \n",
    "        gradapprox[i] = (J_plus - J_minus) / (2 * epsilon)\n",
    "        \n",
    "    num = np.linalg.norm(gradient - gradapprox)\n",
    "    den = np.linalg.norm(gradient) + np.linalg.norm(gradapprox)\n",
    "    diff = num / den\n",
    "    \n",
    "    return diff, gradapprox, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (4, 10)\n",
      "Y: (1, 10)\n",
      "0.6932442608132339\n",
      "\u001b[92m\n",
      "1.8416363538138712e-09\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X = np.random.randn(4, 10)\n",
    "Y = np.mean(X, axis = 0) + np.random.randn(1, 10)\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"Y:\", Y.shape)\n",
    "\n",
    "layer_dims = [4, 3, 1]\n",
    "layer_activations = [None, 'r', 's']\n",
    "\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A, caches = forward_propagation(layer_dims, layer_activations, X, parameters)\n",
    "print(compute_cost(Y, A))\n",
    "gradients = backward_propagation(layer_dims, layer_activations, A, Y, caches, parameters)\n",
    "\n",
    "np.random.seed()\n",
    "random_data_point = int(np.random.rand() * X.shape[1])\n",
    "#diff, grad_ax, grad_bp = gradient_check(X[:, random_data_point].reshape((X.shape[0], 1)), Y[:, random_data_point].reshape((Y.shape[0], 1)), layer_dims, layer_activations, parameters, gradients)\n",
    "diff, _, _ = gradient_check(X, Y, layer_dims, layer_activations, parameters, gradients)\n",
    "\n",
    "if diff > 2e-7:\n",
    "    print(\"\\033[91m\")\n",
    "else:\n",
    "    print(\"\\033[92m\")\n",
    "print(str(diff))\n",
    "print(\"\\033[0m\")\n",
    "\n",
    "#print(\"Approx:\", grad_ax)\n",
    "#print(\"Backprop:\", grad_bp)\n",
    "\n",
    "del X, Y, layer_dims, layer_activations, parameters, A, caches, gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "To train the model we need the following inputs:\n",
    "\n",
    "1. Training dataset - `X_train`, `Y_train`\n",
    "1. Model structure definition - `layer_dims`, `layer_activations`\n",
    "3. Choice of Optimization function - one of `['GD', 'GD_Mom', 'RMSProp', 'Adam']`\n",
    "4. Learning Rate - `learning_rate` ($\\alpha$) (default = 0.0007)\n",
    "5. Size of mini batches - `mini_batch_size` (default = 64)\n",
    "6. Optimizer hyperparameters -\n",
    "    - `beta1` ($\\beta_1$) (default = 0.9)\n",
    "    - `beta2` ($\\beta_2$) (default = 0.999)\n",
    "    - `epsilon` ($\\epsilon$) (default = 1e-8)\n",
    "5. Number of epochs (1 epoch = 1 pass through complete dataset) - `num_epochs` (default = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dims, layer_activations, optimizer, learning_rate = 0.0007, mini_batch_size = 64, \n",
    "          beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True, enable_gradcheck = False):\n",
    "    \n",
    "    assert(len(layer_dims) == len(layer_activations) + 1)\n",
    "    assert(all(activation in activations for activation in layer_activations))\n",
    "    \n",
    "    wandb.init(project = \"dnn-impl\", config = {\n",
    "        'learning_rate': learning_rate\n",
    "    })\n",
    "        \n",
    "    layer_activations = [None] + layer_activations\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    costs = []\n",
    "    t = 0\n",
    "    seed = 21\n",
    "    m = X.shape[1]\n",
    "    grad_check = []\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    # Initialize optimizer moments\n",
    "    if optimizer == 'GD':\n",
    "        pass\n",
    "    elif optimizer == 'GD_Mom':\n",
    "        V = initialize_momentum(layer_dims)\n",
    "    elif optimizer == 'RMSProp':\n",
    "        S = initialize_RMSprop(layer_dims)\n",
    "    elif optimizer == 'Adam':\n",
    "        V, S = initialize_Adam(layer_dims)\n",
    "        \n",
    "    # Training (optimization) loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Generate random mini-batches for each epoch\n",
    "        seed += 1\n",
    "        minibatches = generate_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        gradcheck_rand_minibatch = np.random.randint(len(minibatches))\n",
    "        \n",
    "        # Loop through all mini-batches\n",
    "        for minibatch_num, minibatch in enumerate(minibatches):\n",
    "            \n",
    "            # Get the X and Y values of the minibatch\n",
    "            minibatch_X, minibatch_Y = minibatch\n",
    "            \n",
    "            # Forward Propagation\n",
    "            A, caches = forward_propagation(layer_dims, layer_activations, minibatch_X, parameters)\n",
    "            \n",
    "            # Calculate Cost\n",
    "            minibatch_cost = compute_cost(minibatch_Y, A)\n",
    "            cost_total += minibatch_cost\n",
    "            \n",
    "            # Backward Propagation\n",
    "            gradients = backward_propagation(layer_dims, layer_activations, A, minibatch_Y, caches, parameters)\n",
    "            \n",
    "            # Gradient Checking (if enabled)\n",
    "            if enable_gradcheck and i % 997 == 0 and gradcheck_rand_minibatch == minibatch_num:\n",
    "                diff, _, _ = gradient_check(minibatch_X, minibatch_Y, layer_dims, layer_activations, parameters, gradients)\n",
    "                wandb.log({'grad_check': diff})\n",
    "                grad_check.append(diff)\n",
    "            \n",
    "            \n",
    "            # Optimization update\n",
    "            t += 1 # iterations counter for Adam\n",
    "            if optimizer == 'GD':\n",
    "                parameters = GD_update(layer_dims, parameters, gradients, learning_rate)\n",
    "            elif optimizer == 'GD_Mom':\n",
    "                parameters, V = GD_momentum_update(layer_dims, parameters, gradients, V, beta1, learning_rate)\n",
    "            elif optimizer == 'RMSProp':\n",
    "                parameters, S = RMSprop_update(layer_dims, parameters, gradients, S, beta2, learning_rate)\n",
    "            elif optimizer == 'Adam':\n",
    "                parameters, V, S = Adam_update(layer_dims, parameters, gradients, V, S, t, \n",
    "                            learning_rate, beta1, beta2, epsilon)\n",
    "            \n",
    "            if num_epochs < 500:\n",
    "                wandb.log({'cost': minibatch_cost / minibatch_X.shape[1]})\n",
    "            \n",
    "        # Average the cost over the dataset\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        # Print the cost every 1000 epochs\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print('Cost after epoch {}: {}'.format(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            wandb.log({'epoch_cost': cost_avg})\n",
    "            costs.append(cost_avg)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction using trained model\n",
    "\n",
    "To predict we execute part of forward propagation without retrieving the caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(layer_dims, layer_activations, X, parameters):\n",
    "    A = X\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        # Get parameters for current layer\n",
    "        A_prev = A\n",
    "        W = parameters['W'][l]\n",
    "        b = parameters['b'][l]\n",
    "        activation = layer_activations[l-1]\n",
    "        \n",
    "        # Compute Activations and caches\n",
    "        A, _ = forward_propagation_layer(A_prev, W, b, activation)\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test !!\n",
    "\n",
    "We'll test our neural network with the `2017_Financial_Data.csv` dataset included with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>...</th>\n",
       "      <th>Receivables growth</th>\n",
       "      <th>Inventory Growth</th>\n",
       "      <th>Asset Growth</th>\n",
       "      <th>Book Value per Share Growth</th>\n",
       "      <th>Debt Growth</th>\n",
       "      <th>R&amp;D Expense Growth</th>\n",
       "      <th>SG&amp;A Expenses Growth</th>\n",
       "      <th>Sector</th>\n",
       "      <th>2018 PRICE VAR [%]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PG</td>\n",
       "      <td>6.505800e+10</td>\n",
       "      <td>-0.0037</td>\n",
       "      <td>3.263800e+10</td>\n",
       "      <td>3.242000e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.865400e+10</td>\n",
       "      <td>1.865400e+10</td>\n",
       "      <td>1.376600e+10</td>\n",
       "      <td>4.650000e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>-0.0195</td>\n",
       "      <td>-0.0529</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0156</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>4.975151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIPS</td>\n",
       "      <td>1.102060e+10</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>8.557810e+09</td>\n",
       "      <td>2.462794e+09</td>\n",
       "      <td>2.733452e+08</td>\n",
       "      <td>8.201852e+08</td>\n",
       "      <td>2.056136e+09</td>\n",
       "      <td>4.066575e+08</td>\n",
       "      <td>1.245995e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9219</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.5889</td>\n",
       "      <td>1.5821</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.2141</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>-56.320000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KR</td>\n",
       "      <td>1.153370e+11</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>8.950200e+10</td>\n",
       "      <td>2.583500e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.916200e+10</td>\n",
       "      <td>2.238300e+10</td>\n",
       "      <td>3.452000e+09</td>\n",
       "      <td>5.220000e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0490</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>-0.990449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAD</td>\n",
       "      <td>2.292754e+10</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>1.786283e+10</td>\n",
       "      <td>5.064707e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.776995e+09</td>\n",
       "      <td>4.816124e+09</td>\n",
       "      <td>2.485830e+08</td>\n",
       "      <td>2.000650e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1063</td>\n",
       "      <td>-0.3365</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>-0.5295</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>-66.666666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GIS</td>\n",
       "      <td>1.561980e+10</td>\n",
       "      <td>-0.0570</td>\n",
       "      <td>1.005200e+10</td>\n",
       "      <td>5.567800e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.888800e+09</td>\n",
       "      <td>3.069200e+09</td>\n",
       "      <td>2.498600e+09</td>\n",
       "      <td>2.951000e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>-0.0943</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0738</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>-31.280412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0       Revenue  Revenue Growth  Cost of Revenue  Gross Profit  \\\n",
       "0         PG  6.505800e+10         -0.0037     3.263800e+10  3.242000e+10   \n",
       "1       VIPS  1.102060e+10          0.3525     8.557810e+09  2.462794e+09   \n",
       "2         KR  1.153370e+11          0.0501     8.950200e+10  2.583500e+10   \n",
       "3        RAD  2.292754e+10          0.1039     1.786283e+10  5.064707e+09   \n",
       "4        GIS  1.561980e+10         -0.0570     1.005200e+10  5.567800e+09   \n",
       "\n",
       "   R&D Expenses  SG&A Expense  Operating Expenses  Operating Income  \\\n",
       "0  0.000000e+00  1.865400e+10        1.865400e+10      1.376600e+10   \n",
       "1  2.733452e+08  8.201852e+08        2.056136e+09      4.066575e+08   \n",
       "2  0.000000e+00  1.916200e+10        2.238300e+10      3.452000e+09   \n",
       "3  0.000000e+00  4.776995e+09        4.816124e+09      2.485830e+08   \n",
       "4  0.000000e+00  2.888800e+09        3.069200e+09      2.498600e+09   \n",
       "\n",
       "   Interest Expense  ...  Receivables growth  Inventory Growth  Asset Growth  \\\n",
       "0      4.650000e+08  ...              0.0505           -0.0195       -0.0529   \n",
       "1      1.245995e+07  ...              0.9219            0.4764        0.5889   \n",
       "2      5.220000e+08  ...             -0.0490            0.0637        0.0769   \n",
       "3      2.000650e+08  ...              0.1063           -0.3365        0.0281   \n",
       "4      2.951000e+08  ...              0.0509            0.0494        0.0046   \n",
       "\n",
       "   Book Value per Share Growth  Debt Growth  R&D Expense Growth  \\\n",
       "0                       0.0012       0.0325              0.0000   \n",
       "1                       1.5821       0.3805              0.2141   \n",
       "2                       0.0071       0.1654              0.0000   \n",
       "3                       0.0502      -0.5295              0.0000   \n",
       "4                      -0.0943       0.1246              0.0000   \n",
       "\n",
       "   SG&A Expenses Growth              Sector  2018 PRICE VAR [%]  Class  \n",
       "0               -0.0156  Consumer Defensive            4.975151      1  \n",
       "1                0.1920  Consumer Defensive          -56.320000      0  \n",
       "2                0.0678  Consumer Defensive           -0.990449      0  \n",
       "3                0.0427  Consumer Defensive          -66.666666      0  \n",
       "4               -0.0738  Consumer Defensive          -31.280412      0  \n",
       "\n",
       "[5 rows x 225 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/2017_Financial_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>Earnings before Tax</th>\n",
       "      <th>...</th>\n",
       "      <th>3Y Dividend per Share Growth (per Share)</th>\n",
       "      <th>Receivables growth</th>\n",
       "      <th>Inventory Growth</th>\n",
       "      <th>Asset Growth</th>\n",
       "      <th>Book Value per Share Growth</th>\n",
       "      <th>Debt Growth</th>\n",
       "      <th>R&amp;D Expense Growth</th>\n",
       "      <th>SG&amp;A Expenses Growth</th>\n",
       "      <th>2018 PRICE VAR [%]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.435000e+03</td>\n",
       "      <td>4236.000000</td>\n",
       "      <td>4.281000e+03</td>\n",
       "      <td>4.426000e+03</td>\n",
       "      <td>4.173000e+03</td>\n",
       "      <td>4.299000e+03</td>\n",
       "      <td>4.299000e+03</td>\n",
       "      <td>4.456000e+03</td>\n",
       "      <td>4.283000e+03</td>\n",
       "      <td>4.381000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>3877.000000</td>\n",
       "      <td>4257.000000</td>\n",
       "      <td>4149.00000</td>\n",
       "      <td>4168.000000</td>\n",
       "      <td>4067.00000</td>\n",
       "      <td>4106.000000</td>\n",
       "      <td>4110.000000</td>\n",
       "      <td>4132.000000</td>\n",
       "      <td>4960.000000</td>\n",
       "      <td>4960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.085608e+09</td>\n",
       "      <td>0.953124</td>\n",
       "      <td>3.235881e+09</td>\n",
       "      <td>1.934378e+09</td>\n",
       "      <td>1.061590e+08</td>\n",
       "      <td>8.532075e+08</td>\n",
       "      <td>1.331302e+09</td>\n",
       "      <td>5.943889e+08</td>\n",
       "      <td>9.961462e+07</td>\n",
       "      <td>4.974142e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>9.660352</td>\n",
       "      <td>0.11746</td>\n",
       "      <td>1.084989</td>\n",
       "      <td>0.22719</td>\n",
       "      <td>1.457853</td>\n",
       "      <td>0.383984</td>\n",
       "      <td>0.719938</td>\n",
       "      <td>-4.461547</td>\n",
       "      <td>0.276210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.148389e+10</td>\n",
       "      <td>18.956861</td>\n",
       "      <td>2.619288e+10</td>\n",
       "      <td>7.911822e+09</td>\n",
       "      <td>7.983065e+08</td>\n",
       "      <td>3.521939e+09</td>\n",
       "      <td>5.214527e+09</td>\n",
       "      <td>2.763764e+09</td>\n",
       "      <td>4.185841e+08</td>\n",
       "      <td>2.362747e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256422</td>\n",
       "      <td>370.977527</td>\n",
       "      <td>1.07944</td>\n",
       "      <td>32.740851</td>\n",
       "      <td>4.59341</td>\n",
       "      <td>33.852958</td>\n",
       "      <td>13.378255</td>\n",
       "      <td>31.830424</td>\n",
       "      <td>561.802215</td>\n",
       "      <td>0.447167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.004500e+07</td>\n",
       "      <td>-6.872700</td>\n",
       "      <td>-2.986888e+09</td>\n",
       "      <td>-8.138230e+08</td>\n",
       "      <td>-1.098000e+08</td>\n",
       "      <td>-1.043667e+08</td>\n",
       "      <td>-1.088000e+09</td>\n",
       "      <td>-1.868300e+10</td>\n",
       "      <td>-1.932443e+08</td>\n",
       "      <td>-1.819800e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-221.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.218100</td>\n",
       "      <td>-2.369900</td>\n",
       "      <td>-99.999115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.900000e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.945000e+06</td>\n",
       "      <td>3.200000e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.861650e+07</td>\n",
       "      <td>3.700850e+07</td>\n",
       "      <td>-4.376053e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.396000e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.008050</td>\n",
       "      <td>-0.06225</td>\n",
       "      <td>-0.059250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003925</td>\n",
       "      <td>-33.388915</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.400000e+08</td>\n",
       "      <td>0.073050</td>\n",
       "      <td>1.588160e+08</td>\n",
       "      <td>1.993761e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.360500e+07</td>\n",
       "      <td>1.638000e+08</td>\n",
       "      <td>3.739450e+07</td>\n",
       "      <td>4.800000e+06</td>\n",
       "      <td>2.464000e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.06060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>-13.336397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.254111e+09</td>\n",
       "      <td>0.211625</td>\n",
       "      <td>1.170103e+09</td>\n",
       "      <td>8.823762e+08</td>\n",
       "      <td>1.400000e+07</td>\n",
       "      <td>3.711330e+08</td>\n",
       "      <td>6.229360e+08</td>\n",
       "      <td>2.604446e+08</td>\n",
       "      <td>5.300000e+07</td>\n",
       "      <td>1.976460e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.09070</td>\n",
       "      <td>0.203425</td>\n",
       "      <td>0.19230</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>2.124593</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.695864e+12</td>\n",
       "      <td>825.959800</td>\n",
       "      <td>1.465577e+12</td>\n",
       "      <td>2.302870e+11</td>\n",
       "      <td>2.262000e+10</td>\n",
       "      <td>1.018530e+11</td>\n",
       "      <td>1.018530e+11</td>\n",
       "      <td>6.513565e+10</td>\n",
       "      <td>1.340100e+10</td>\n",
       "      <td>6.408900e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>18499.000000</td>\n",
       "      <td>43.55340</td>\n",
       "      <td>2014.273300</td>\n",
       "      <td>84.12770</td>\n",
       "      <td>1981.061600</td>\n",
       "      <td>837.000000</td>\n",
       "      <td>2042.000000</td>\n",
       "      <td>39219.999109</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Revenue  Revenue Growth  Cost of Revenue  Gross Profit  \\\n",
       "count  4.435000e+03     4236.000000     4.281000e+03  4.426000e+03   \n",
       "mean   5.085608e+09        0.953124     3.235881e+09  1.934378e+09   \n",
       "std    3.148389e+10       18.956861     2.619288e+10  7.911822e+09   \n",
       "min   -3.004500e+07       -6.872700    -2.986888e+09 -8.138230e+08   \n",
       "25%    5.900000e+07        0.000000     3.945000e+06  3.200000e+07   \n",
       "50%    4.400000e+08        0.073050     1.588160e+08  1.993761e+08   \n",
       "75%    2.254111e+09        0.211625     1.170103e+09  8.823762e+08   \n",
       "max    1.695864e+12      825.959800     1.465577e+12  2.302870e+11   \n",
       "\n",
       "       R&D Expenses  SG&A Expense  Operating Expenses  Operating Income  \\\n",
       "count  4.173000e+03  4.299000e+03        4.299000e+03      4.456000e+03   \n",
       "mean   1.061590e+08  8.532075e+08        1.331302e+09      5.943889e+08   \n",
       "std    7.983065e+08  3.521939e+09        5.214527e+09      2.763764e+09   \n",
       "min   -1.098000e+08 -1.043667e+08       -1.088000e+09     -1.868300e+10   \n",
       "25%    0.000000e+00  1.861650e+07        3.700850e+07     -4.376053e+06   \n",
       "50%    0.000000e+00  8.360500e+07        1.638000e+08      3.739450e+07   \n",
       "75%    1.400000e+07  3.711330e+08        6.229360e+08      2.604446e+08   \n",
       "max    2.262000e+10  1.018530e+11        1.018530e+11      6.513565e+10   \n",
       "\n",
       "       Interest Expense  Earnings before Tax  ...  \\\n",
       "count      4.283000e+03         4.381000e+03  ...   \n",
       "mean       9.961462e+07         4.974142e+08  ...   \n",
       "std        4.185841e+08         2.362747e+09  ...   \n",
       "min       -1.932443e+08        -1.819800e+10  ...   \n",
       "25%        0.000000e+00        -9.396000e+06  ...   \n",
       "50%        4.800000e+06         2.464000e+07  ...   \n",
       "75%        5.300000e+07         1.976460e+08  ...   \n",
       "max        1.340100e+10         6.408900e+10  ...   \n",
       "\n",
       "       3Y Dividend per Share Growth (per Share)  Receivables growth  \\\n",
       "count                               3877.000000         4257.000000   \n",
       "mean                                   0.005444            9.660352   \n",
       "std                                    0.256422          370.977527   \n",
       "min                                   -1.000000           -1.000000   \n",
       "25%                                    0.000000           -0.003000   \n",
       "50%                                    0.000000            0.054600   \n",
       "75%                                    0.037100            0.249200   \n",
       "max                                    3.561000        18499.000000   \n",
       "\n",
       "       Inventory Growth  Asset Growth  Book Value per Share Growth  \\\n",
       "count        4149.00000   4168.000000                   4067.00000   \n",
       "mean            0.11746      1.084989                      0.22719   \n",
       "std             1.07944     32.740851                      4.59341   \n",
       "min            -1.00000     -1.000000                   -221.00000   \n",
       "25%             0.00000     -0.008050                     -0.06225   \n",
       "50%             0.00000      0.066300                      0.06060   \n",
       "75%             0.09070      0.203425                      0.19230   \n",
       "max            43.55340   2014.273300                     84.12770   \n",
       "\n",
       "       Debt Growth  R&D Expense Growth  SG&A Expenses Growth  \\\n",
       "count  4106.000000         4110.000000           4132.000000   \n",
       "mean      1.457853            0.383984              0.719938   \n",
       "std      33.852958           13.378255             31.830424   \n",
       "min      -1.000000           -1.218100             -2.369900   \n",
       "25%      -0.059250            0.000000             -0.003925   \n",
       "50%       0.000000            0.000000              0.067250   \n",
       "75%       0.152600            0.001175              0.182500   \n",
       "max    1981.061600          837.000000           2042.000000   \n",
       "\n",
       "       2018 PRICE VAR [%]        Class  \n",
       "count         4960.000000  4960.000000  \n",
       "mean            -4.461547     0.276210  \n",
       "std            561.802215     0.447167  \n",
       "min            -99.999115     0.000000  \n",
       "25%            -33.388915     0.000000  \n",
       "50%            -13.336397     0.000000  \n",
       "75%              2.124593     1.000000  \n",
       "max          39219.999109     1.000000  \n",
       "\n",
       "[8 rows x 223 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    "Before we can use the data with our Neural Network, we will have to preprocess it to make it suitable for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Extract features and labels\n",
    "\n",
    "First we will extract the Input features and the Labels.\n",
    "- Here, the last column `'Class'` is the **output**\n",
    "    - Separate it into `Y` vector\n",
    "- The first column `'Unnamed: 0'` is not useful as it only mentions the id for the stocks\n",
    "    - Drop column 0\n",
    "- Among the input columns, we have one _Categorical_ feature - `'Sector'`\n",
    "    - Convert to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate labels into Y\n",
    "Y = df['Class']\n",
    "\n",
    "# Drop column 'Unnamed: 0\" (and 'Class' as it has been separated out)\n",
    "ids = df['Unnamed: 0'] # saving ids for later\n",
    "X = df.drop(df.columns[[0, -1]], axis=1)\n",
    "\n",
    "# Delete df to save memory\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_Basic Materials</th>\n",
       "      <th>Sector_Communication Services</th>\n",
       "      <th>Sector_Consumer Cyclical</th>\n",
       "      <th>Sector_Consumer Defensive</th>\n",
       "      <th>Sector_Energy</th>\n",
       "      <th>Sector_Financial Services</th>\n",
       "      <th>Sector_Healthcare</th>\n",
       "      <th>Sector_Industrials</th>\n",
       "      <th>Sector_Real Estate</th>\n",
       "      <th>Sector_Technology</th>\n",
       "      <th>Sector_Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4960 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sector_Basic Materials  Sector_Communication Services  \\\n",
       "0                        0.0                            0.0   \n",
       "1                        0.0                            0.0   \n",
       "2                        0.0                            0.0   \n",
       "3                        0.0                            0.0   \n",
       "4                        0.0                            0.0   \n",
       "...                      ...                            ...   \n",
       "4955                     0.0                            0.0   \n",
       "4956                     0.0                            0.0   \n",
       "4957                     0.0                            0.0   \n",
       "4958                     0.0                            0.0   \n",
       "4959                     0.0                            0.0   \n",
       "\n",
       "      Sector_Consumer Cyclical  Sector_Consumer Defensive  Sector_Energy  \\\n",
       "0                          0.0                        1.0            0.0   \n",
       "1                          0.0                        1.0            0.0   \n",
       "2                          0.0                        1.0            0.0   \n",
       "3                          0.0                        1.0            0.0   \n",
       "4                          0.0                        1.0            0.0   \n",
       "...                        ...                        ...            ...   \n",
       "4955                       0.0                        0.0            0.0   \n",
       "4956                       0.0                        0.0            0.0   \n",
       "4957                       0.0                        0.0            0.0   \n",
       "4958                       0.0                        0.0            0.0   \n",
       "4959                       0.0                        0.0            0.0   \n",
       "\n",
       "      Sector_Financial Services  Sector_Healthcare  Sector_Industrials  \\\n",
       "0                           0.0                0.0                 0.0   \n",
       "1                           0.0                0.0                 0.0   \n",
       "2                           0.0                0.0                 0.0   \n",
       "3                           0.0                0.0                 0.0   \n",
       "4                           0.0                0.0                 0.0   \n",
       "...                         ...                ...                 ...   \n",
       "4955                        0.0                0.0                 0.0   \n",
       "4956                        0.0                0.0                 0.0   \n",
       "4957                        0.0                0.0                 0.0   \n",
       "4958                        0.0                0.0                 0.0   \n",
       "4959                        0.0                0.0                 0.0   \n",
       "\n",
       "      Sector_Real Estate  Sector_Technology  Sector_Utilities  \n",
       "0                    0.0                0.0               0.0  \n",
       "1                    0.0                0.0               0.0  \n",
       "2                    0.0                0.0               0.0  \n",
       "3                    0.0                0.0               0.0  \n",
       "4                    0.0                0.0               0.0  \n",
       "...                  ...                ...               ...  \n",
       "4955                 0.0                1.0               0.0  \n",
       "4956                 0.0                1.0               0.0  \n",
       "4957                 0.0                1.0               0.0  \n",
       "4958                 0.0                1.0               0.0  \n",
       "4959                 0.0                1.0               0.0  \n",
       "\n",
       "[4960 rows x 11 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical column to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "onehotdf = encoder.fit_transform(X.Sector.values.reshape(-1,1)).toarray()\n",
    "onehotdf = pd.DataFrame(onehotdf, columns = encoder.get_feature_names(['Sector']))\n",
    "onehotdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>Earnings before Tax</th>\n",
       "      <th>...</th>\n",
       "      <th>Sector_Communication Services</th>\n",
       "      <th>Sector_Consumer Cyclical</th>\n",
       "      <th>Sector_Consumer Defensive</th>\n",
       "      <th>Sector_Energy</th>\n",
       "      <th>Sector_Financial Services</th>\n",
       "      <th>Sector_Healthcare</th>\n",
       "      <th>Sector_Industrials</th>\n",
       "      <th>Sector_Real Estate</th>\n",
       "      <th>Sector_Technology</th>\n",
       "      <th>Sector_Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.505800e+10</td>\n",
       "      <td>-0.0037</td>\n",
       "      <td>3.263800e+10</td>\n",
       "      <td>3.242000e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.865400e+10</td>\n",
       "      <td>1.865400e+10</td>\n",
       "      <td>1.376600e+10</td>\n",
       "      <td>4.650000e+08</td>\n",
       "      <td>1.838900e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.102060e+10</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>8.557810e+09</td>\n",
       "      <td>2.462794e+09</td>\n",
       "      <td>2.733452e+08</td>\n",
       "      <td>8.201852e+08</td>\n",
       "      <td>2.056136e+09</td>\n",
       "      <td>4.066575e+08</td>\n",
       "      <td>1.245995e+07</td>\n",
       "      <td>3.893281e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.153370e+11</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>8.950200e+10</td>\n",
       "      <td>2.583500e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.916200e+10</td>\n",
       "      <td>2.238300e+10</td>\n",
       "      <td>3.452000e+09</td>\n",
       "      <td>5.220000e+08</td>\n",
       "      <td>2.932000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.292754e+10</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>1.786283e+10</td>\n",
       "      <td>5.064707e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.776995e+09</td>\n",
       "      <td>4.816124e+09</td>\n",
       "      <td>2.485830e+08</td>\n",
       "      <td>2.000650e+08</td>\n",
       "      <td>4.849100e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.561980e+10</td>\n",
       "      <td>-0.0570</td>\n",
       "      <td>1.005200e+10</td>\n",
       "      <td>5.567800e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.888800e+09</td>\n",
       "      <td>3.069200e+09</td>\n",
       "      <td>2.498600e+09</td>\n",
       "      <td>2.951000e+08</td>\n",
       "      <td>2.312700e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>6.257258e+07</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>5.232652e+07</td>\n",
       "      <td>1.024606e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.683601e+06</td>\n",
       "      <td>9.683601e+06</td>\n",
       "      <td>5.624630e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.311890e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>1.065240e+08</td>\n",
       "      <td>-0.0677</td>\n",
       "      <td>1.290900e+07</td>\n",
       "      <td>9.361500e+07</td>\n",
       "      <td>9.224000e+06</td>\n",
       "      <td>7.984600e+07</td>\n",
       "      <td>8.907000e+07</td>\n",
       "      <td>4.545000e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.656000e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>1.040000e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.700000e+07</td>\n",
       "      <td>2.700000e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>2.700000e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>-2.000000e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>1.605670e+08</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>1.334910e+08</td>\n",
       "      <td>2.707600e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.926300e+07</td>\n",
       "      <td>1.926300e+07</td>\n",
       "      <td>7.813000e+06</td>\n",
       "      <td>-6.990000e+05</td>\n",
       "      <td>8.553000e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>4.607800e+07</td>\n",
       "      <td>0.4709</td>\n",
       "      <td>2.681700e+07</td>\n",
       "      <td>1.926100e+07</td>\n",
       "      <td>4.395000e+06</td>\n",
       "      <td>1.798700e+07</td>\n",
       "      <td>2.238200e+07</td>\n",
       "      <td>-3.121000e+06</td>\n",
       "      <td>2.960000e+05</td>\n",
       "      <td>-3.246000e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4960 rows × 233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Revenue  Revenue Growth  Cost of Revenue  Gross Profit  \\\n",
       "0     6.505800e+10         -0.0037     3.263800e+10  3.242000e+10   \n",
       "1     1.102060e+10          0.3525     8.557810e+09  2.462794e+09   \n",
       "2     1.153370e+11          0.0501     8.950200e+10  2.583500e+10   \n",
       "3     2.292754e+10          0.1039     1.786283e+10  5.064707e+09   \n",
       "4     1.561980e+10         -0.0570     1.005200e+10  5.567800e+09   \n",
       "...            ...             ...              ...           ...   \n",
       "4955  6.257258e+07          0.0258     5.232652e+07  1.024606e+07   \n",
       "4956  1.065240e+08         -0.0677     1.290900e+07  9.361500e+07   \n",
       "4957  1.040000e+08             NaN     7.700000e+07  2.700000e+07   \n",
       "4958  1.605670e+08         -0.0246     1.334910e+08  2.707600e+07   \n",
       "4959  4.607800e+07          0.4709     2.681700e+07  1.926100e+07   \n",
       "\n",
       "      R&D Expenses  SG&A Expense  Operating Expenses  Operating Income  \\\n",
       "0     0.000000e+00  1.865400e+10        1.865400e+10      1.376600e+10   \n",
       "1     2.733452e+08  8.201852e+08        2.056136e+09      4.066575e+08   \n",
       "2     0.000000e+00  1.916200e+10        2.238300e+10      3.452000e+09   \n",
       "3     0.000000e+00  4.776995e+09        4.816124e+09      2.485830e+08   \n",
       "4     0.000000e+00  2.888800e+09        3.069200e+09      2.498600e+09   \n",
       "...            ...           ...                 ...               ...   \n",
       "4955  0.000000e+00  9.683601e+06        9.683601e+06      5.624630e+05   \n",
       "4956  9.224000e+06  7.984600e+07        8.907000e+07      4.545000e+06   \n",
       "4957           NaN  2.500000e+07        2.700000e+07      0.000000e+00   \n",
       "4958  0.000000e+00  1.926300e+07        1.926300e+07      7.813000e+06   \n",
       "4959  4.395000e+06  1.798700e+07        2.238200e+07     -3.121000e+06   \n",
       "\n",
       "      Interest Expense  Earnings before Tax  ...  \\\n",
       "0         4.650000e+08         1.838900e+10  ...   \n",
       "1         1.245995e+07         3.893281e+08  ...   \n",
       "2         5.220000e+08         2.932000e+09  ...   \n",
       "3         2.000650e+08         4.849100e+07  ...   \n",
       "4         2.951000e+08         2.312700e+09  ...   \n",
       "...                ...                  ...  ...   \n",
       "4955      0.000000e+00         5.311890e+05  ...   \n",
       "4956      0.000000e+00         6.656000e+06  ...   \n",
       "4957      1.000000e+06        -2.000000e+06  ...   \n",
       "4958     -6.990000e+05         8.553000e+06  ...   \n",
       "4959      2.960000e+05        -3.246000e+06  ...   \n",
       "\n",
       "      Sector_Communication Services  Sector_Consumer Cyclical  \\\n",
       "0                               0.0                       0.0   \n",
       "1                               0.0                       0.0   \n",
       "2                               0.0                       0.0   \n",
       "3                               0.0                       0.0   \n",
       "4                               0.0                       0.0   \n",
       "...                             ...                       ...   \n",
       "4955                            0.0                       0.0   \n",
       "4956                            0.0                       0.0   \n",
       "4957                            0.0                       0.0   \n",
       "4958                            0.0                       0.0   \n",
       "4959                            0.0                       0.0   \n",
       "\n",
       "      Sector_Consumer Defensive  Sector_Energy  Sector_Financial Services  \\\n",
       "0                           1.0            0.0                        0.0   \n",
       "1                           1.0            0.0                        0.0   \n",
       "2                           1.0            0.0                        0.0   \n",
       "3                           1.0            0.0                        0.0   \n",
       "4                           1.0            0.0                        0.0   \n",
       "...                         ...            ...                        ...   \n",
       "4955                        0.0            0.0                        0.0   \n",
       "4956                        0.0            0.0                        0.0   \n",
       "4957                        0.0            0.0                        0.0   \n",
       "4958                        0.0            0.0                        0.0   \n",
       "4959                        0.0            0.0                        0.0   \n",
       "\n",
       "      Sector_Healthcare  Sector_Industrials  Sector_Real Estate  \\\n",
       "0                   0.0                 0.0                 0.0   \n",
       "1                   0.0                 0.0                 0.0   \n",
       "2                   0.0                 0.0                 0.0   \n",
       "3                   0.0                 0.0                 0.0   \n",
       "4                   0.0                 0.0                 0.0   \n",
       "...                 ...                 ...                 ...   \n",
       "4955                0.0                 0.0                 0.0   \n",
       "4956                0.0                 0.0                 0.0   \n",
       "4957                0.0                 0.0                 0.0   \n",
       "4958                0.0                 0.0                 0.0   \n",
       "4959                0.0                 0.0                 0.0   \n",
       "\n",
       "      Sector_Technology  Sector_Utilities  \n",
       "0                   0.0               0.0  \n",
       "1                   0.0               0.0  \n",
       "2                   0.0               0.0  \n",
       "3                   0.0               0.0  \n",
       "4                   0.0               0.0  \n",
       "...                 ...               ...  \n",
       "4955                1.0               0.0  \n",
       "4956                1.0               0.0  \n",
       "4957                1.0               0.0  \n",
       "4958                1.0               0.0  \n",
       "4959                1.0               0.0  \n",
       "\n",
       "[4960 rows x 233 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add one-hot encoding to the Input data and remove old column\n",
    "X = pd.concat([X, onehotdf], axis=1)\n",
    "X = X.drop(['Sector'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Fill in missing (NaN) values\n",
    "\n",
    "The data contains some missing values which are depicted as `NaN`. This will lead to confounding in our model and cause Arithmetic errors.\n",
    "\n",
    "We can either drop these rows or fill them with feasible values (like the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>Earnings before Tax</th>\n",
       "      <th>...</th>\n",
       "      <th>Sector_Communication Services</th>\n",
       "      <th>Sector_Consumer Cyclical</th>\n",
       "      <th>Sector_Consumer Defensive</th>\n",
       "      <th>Sector_Energy</th>\n",
       "      <th>Sector_Financial Services</th>\n",
       "      <th>Sector_Healthcare</th>\n",
       "      <th>Sector_Industrials</th>\n",
       "      <th>Sector_Real Estate</th>\n",
       "      <th>Sector_Technology</th>\n",
       "      <th>Sector_Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Revenue, Revenue Growth, Cost of Revenue, Gross Profit, R&D Expenses, SG&A Expense, Operating Expenses, Operating Income, Interest Expense, Earnings before Tax, Income Tax Expense, Net Income - Non-Controlling int, Net Income - Discontinued ops, Net Income, Preferred Dividends, Net Income Com, EPS, EPS Diluted, Weighted Average Shs Out, Weighted Average Shs Out (Dil), Dividend per Share, Gross Margin, EBITDA Margin, EBIT Margin, Profit Margin, Free Cash Flow margin, EBITDA, EBIT, Consolidated Income, Earnings Before Tax Margin, Net Profit Margin, Cash and cash equivalents, Short-term investments, Cash and short-term investments, Receivables, Inventories, Total current assets, Property, Plant & Equipment Net, Goodwill and Intangible Assets, Long-term investments, Tax assets, Total non-current assets, Total assets, Payables, Short-term debt, Total current liabilities, Long-term debt, Total debt, Deferred revenue, Tax Liabilities, Deposit Liabilities, Total non-current liabilities, Total liabilities, Other comprehensive income, Retained earnings (deficit), Total shareholders equity, Investments, Net Debt, Other Assets, Other Liabilities, Depreciation & Amortization, Stock-based compensation, Operating Cash Flow, Capital Expenditure, Acquisitions and disposals, Investment purchases and sales, Investing Cash flow, Issuance (repayment) of debt, Issuance (buybacks) of shares, Dividend payments, Financing Cash Flow, Effect of forex changes on cash, Net cash flow / Change in cash, Free Cash Flow, Net Cash/Marketcap, priceBookValueRatio, priceToBookRatio, priceToSalesRatio, priceEarningsRatio, priceToFreeCashFlowsRatio, priceToOperatingCashFlowsRatio, priceCashFlowRatio, priceEarningsToGrowthRatio, priceSalesRatio, dividendYield, enterpriseValueMultiple, priceFairValue, ebitperRevenue, ebtperEBIT, niperEBT, grossProfitMargin, operatingProfitMargin, pretaxProfitMargin, netProfitMargin, effectiveTaxRate, returnOnAssets, returnOnEquity, returnOnCapitalEmployed, nIperEBT, eBTperEBIT, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 233 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X.isnull().sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that the data set does not have any row which is completely free of `NaN`/null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2018 PRICE VAR [%]', 'Sector_Basic Materials',\n",
       "       'Sector_Communication Services', 'Sector_Consumer Cyclical',\n",
       "       'Sector_Consumer Defensive', 'Sector_Energy',\n",
       "       'Sector_Financial Services', 'Sector_Healthcare', 'Sector_Industrials',\n",
       "       'Sector_Real Estate', 'Sector_Technology', 'Sector_Utilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns[X.isnull().sum(axis=0) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also there is only one column, other than the one-hot encoded columns, which is completely free of null values\n",
    "\n",
    "Thus we cannot get rid of the `NaN` values without losing out on important information. So we will fill the `NaN` values with mean values of the respective columns.\n",
    "\n",
    "(NOTE: In-depth study of the dataset may lead to way of dropping columns/rows in order to reduce null values while still retaining sufficient data for training)\n",
    "\n",
    "(NOTE: There may be better ways to fill in the null values but for the sake of simplicity we will just fill in the means here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with no values\n",
    "nullCols = X.columns[X.isnull().sum(axis=0) == len(X.index)]\n",
    "X = X.drop(nullCols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in NaN values with means\n",
    "X = X.fillna(X.mean())\n",
    "X.columns[X.isnull().sum(axis=0) > 0] # To show that all NaN values have been filled (output should be empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.columns[:-11])[((X[X.columns[:-11]] == 0).astype(int).sum(axis=0) / len(X.index)) > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Normalize features\n",
    "\n",
    "As we can see, some of the columns have very large values (in the range of 1e+10) while some have very small values (in the range of 1e-3).\n",
    "\n",
    "Thus we will **Normalize** the data for each feature -\n",
    "\n",
    "$$ x := \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "where $\\mu$ is the Mean, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "NOTE: We should not Normalize the one-hot encoded columns. There are 11 columns in the one-hot encoding. Thus we apply the Normalization for all but the last 11 columns, which can be achieved by using `X[X.columns[:-11]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>Earnings before Tax</th>\n",
       "      <th>...</th>\n",
       "      <th>Sector_Communication Services</th>\n",
       "      <th>Sector_Consumer Cyclical</th>\n",
       "      <th>Sector_Consumer Defensive</th>\n",
       "      <th>Sector_Energy</th>\n",
       "      <th>Sector_Financial Services</th>\n",
       "      <th>Sector_Healthcare</th>\n",
       "      <th>Sector_Industrials</th>\n",
       "      <th>Sector_Real Estate</th>\n",
       "      <th>Sector_Technology</th>\n",
       "      <th>Sector_Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.014476</td>\n",
       "      <td>-5.461804e-02</td>\n",
       "      <td>1.208288</td>\n",
       "      <td>4.079049</td>\n",
       "      <td>-1.449813e-01</td>\n",
       "      <td>5.429020</td>\n",
       "      <td>3.568326</td>\n",
       "      <td>5.028183</td>\n",
       "      <td>0.939382</td>\n",
       "      <td>8.057340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199357</td>\n",
       "      <td>-3.428521e-02</td>\n",
       "      <td>0.218706</td>\n",
       "      <td>0.070703</td>\n",
       "      <td>2.283263e-01</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>0.149310</td>\n",
       "      <td>-0.071665</td>\n",
       "      <td>-0.224069</td>\n",
       "      <td>-0.048676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.703351</td>\n",
       "      <td>-5.154700e-02</td>\n",
       "      <td>3.545130</td>\n",
       "      <td>3.197961</td>\n",
       "      <td>-1.449813e-01</td>\n",
       "      <td>5.583953</td>\n",
       "      <td>4.336467</td>\n",
       "      <td>1.090876</td>\n",
       "      <td>1.085925</td>\n",
       "      <td>1.096397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599312</td>\n",
       "      <td>-4.847595e-02</td>\n",
       "      <td>0.601099</td>\n",
       "      <td>0.418846</td>\n",
       "      <td>-1.449813e-01</td>\n",
       "      <td>1.196706</td>\n",
       "      <td>0.717843</td>\n",
       "      <td>-0.132009</td>\n",
       "      <td>0.258251</td>\n",
       "      <td>-0.202169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.353844</td>\n",
       "      <td>-5.766054e-02</td>\n",
       "      <td>0.280110</td>\n",
       "      <td>0.486161</td>\n",
       "      <td>-1.449813e-01</td>\n",
       "      <td>0.620830</td>\n",
       "      <td>0.357992</td>\n",
       "      <td>0.726921</td>\n",
       "      <td>0.502580</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>-0.168724</td>\n",
       "      <td>-5.293410e-02</td>\n",
       "      <td>-0.130829</td>\n",
       "      <td>-0.257453</td>\n",
       "      <td>-1.449813e-01</td>\n",
       "      <td>-0.257264</td>\n",
       "      <td>-0.272242</td>\n",
       "      <td>-0.226690</td>\n",
       "      <td>-0.256103</td>\n",
       "      <td>-0.223767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>-0.167248</td>\n",
       "      <td>-5.827133e-02</td>\n",
       "      <td>-0.132449</td>\n",
       "      <td>-0.246299</td>\n",
       "      <td>-1.323841e-01</td>\n",
       "      <td>-0.235866</td>\n",
       "      <td>-0.255889</td>\n",
       "      <td>-0.225169</td>\n",
       "      <td>-0.256103</td>\n",
       "      <td>-0.221009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>-0.167333</td>\n",
       "      <td>-8.621835e-17</td>\n",
       "      <td>-0.129815</td>\n",
       "      <td>-0.255212</td>\n",
       "      <td>-9.436849e-16</td>\n",
       "      <td>-0.252593</td>\n",
       "      <td>-0.268675</td>\n",
       "      <td>-0.226904</td>\n",
       "      <td>-0.253532</td>\n",
       "      <td>-0.224907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>-0.165432</td>\n",
       "      <td>-5.581107e-02</td>\n",
       "      <td>-0.127494</td>\n",
       "      <td>-0.255202</td>\n",
       "      <td>-1.449813e-01</td>\n",
       "      <td>-0.254343</td>\n",
       "      <td>-0.270269</td>\n",
       "      <td>-0.223922</td>\n",
       "      <td>-0.257900</td>\n",
       "      <td>-0.220155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>-0.169278</td>\n",
       "      <td>-2.752663e-02</td>\n",
       "      <td>-0.131877</td>\n",
       "      <td>-0.256247</td>\n",
       "      <td>-1.389790e-01</td>\n",
       "      <td>-0.254732</td>\n",
       "      <td>-0.269626</td>\n",
       "      <td>-0.228096</td>\n",
       "      <td>-0.255342</td>\n",
       "      <td>-0.225469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4960 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Revenue  Revenue Growth  Cost of Revenue  Gross Profit  R&D Expenses  \\\n",
       "0     2.014476   -5.461804e-02         1.208288      4.079049 -1.449813e-01   \n",
       "1     0.199357   -3.428521e-02         0.218706      0.070703  2.283263e-01   \n",
       "2     3.703351   -5.154700e-02         3.545130      3.197961 -1.449813e-01   \n",
       "3     0.599312   -4.847595e-02         0.601099      0.418846 -1.449813e-01   \n",
       "4     0.353844   -5.766054e-02         0.280110      0.486161 -1.449813e-01   \n",
       "...        ...             ...              ...           ...           ...   \n",
       "4955 -0.168724   -5.293410e-02        -0.130829     -0.257453 -1.449813e-01   \n",
       "4956 -0.167248   -5.827133e-02        -0.132449     -0.246299 -1.323841e-01   \n",
       "4957 -0.167333   -8.621835e-17        -0.129815     -0.255212 -9.436849e-16   \n",
       "4958 -0.165432   -5.581107e-02        -0.127494     -0.255202 -1.449813e-01   \n",
       "4959 -0.169278   -2.752663e-02        -0.131877     -0.256247 -1.389790e-01   \n",
       "\n",
       "      SG&A Expense  Operating Expenses  Operating Income  Interest Expense  \\\n",
       "0         5.429020            3.568326          5.028183          0.939382   \n",
       "1        -0.010071            0.149310         -0.071665         -0.224069   \n",
       "2         5.583953            4.336467          1.090876          1.085925   \n",
       "3         1.196706            0.717843         -0.132009          0.258251   \n",
       "4         0.620830            0.357992          0.726921          0.502580   \n",
       "...            ...                 ...               ...               ...   \n",
       "4955     -0.257264           -0.272242         -0.226690         -0.256103   \n",
       "4956     -0.235866           -0.255889         -0.225169         -0.256103   \n",
       "4957     -0.252593           -0.268675         -0.226904         -0.253532   \n",
       "4958     -0.254343           -0.270269         -0.223922         -0.257900   \n",
       "4959     -0.254732           -0.269626         -0.228096         -0.255342   \n",
       "\n",
       "      Earnings before Tax  ...  Sector_Communication Services  \\\n",
       "0                8.057340  ...                            0.0   \n",
       "1               -0.048676  ...                            0.0   \n",
       "2                1.096397  ...                            0.0   \n",
       "3               -0.202169  ...                            0.0   \n",
       "4                0.817500  ...                            0.0   \n",
       "...                   ...  ...                            ...   \n",
       "4955            -0.223767  ...                            0.0   \n",
       "4956            -0.221009  ...                            0.0   \n",
       "4957            -0.224907  ...                            0.0   \n",
       "4958            -0.220155  ...                            0.0   \n",
       "4959            -0.225469  ...                            0.0   \n",
       "\n",
       "      Sector_Consumer Cyclical  Sector_Consumer Defensive  Sector_Energy  \\\n",
       "0                          0.0                        1.0            0.0   \n",
       "1                          0.0                        1.0            0.0   \n",
       "2                          0.0                        1.0            0.0   \n",
       "3                          0.0                        1.0            0.0   \n",
       "4                          0.0                        1.0            0.0   \n",
       "...                        ...                        ...            ...   \n",
       "4955                       0.0                        0.0            0.0   \n",
       "4956                       0.0                        0.0            0.0   \n",
       "4957                       0.0                        0.0            0.0   \n",
       "4958                       0.0                        0.0            0.0   \n",
       "4959                       0.0                        0.0            0.0   \n",
       "\n",
       "      Sector_Financial Services  Sector_Healthcare  Sector_Industrials  \\\n",
       "0                           0.0                0.0                 0.0   \n",
       "1                           0.0                0.0                 0.0   \n",
       "2                           0.0                0.0                 0.0   \n",
       "3                           0.0                0.0                 0.0   \n",
       "4                           0.0                0.0                 0.0   \n",
       "...                         ...                ...                 ...   \n",
       "4955                        0.0                0.0                 0.0   \n",
       "4956                        0.0                0.0                 0.0   \n",
       "4957                        0.0                0.0                 0.0   \n",
       "4958                        0.0                0.0                 0.0   \n",
       "4959                        0.0                0.0                 0.0   \n",
       "\n",
       "      Sector_Real Estate  Sector_Technology  Sector_Utilities  \n",
       "0                    0.0                0.0               0.0  \n",
       "1                    0.0                0.0               0.0  \n",
       "2                    0.0                0.0               0.0  \n",
       "3                    0.0                0.0               0.0  \n",
       "4                    0.0                0.0               0.0  \n",
       "...                  ...                ...               ...  \n",
       "4955                 0.0                1.0               0.0  \n",
       "4956                 0.0                1.0               0.0  \n",
       "4957                 0.0                1.0               0.0  \n",
       "4958                 0.0                1.0               0.0  \n",
       "4959                 0.0                1.0               0.0  \n",
       "\n",
       "[4960 rows x 232 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mean and std for each column\n",
    "X_mean = X[X.columns[:-11]].mean()\n",
    "X_std = X[X.columns[:-11]].std()\n",
    "\n",
    "# Normalize columns\n",
    "X[X.columns[:-11]] = (X[X.columns[:-11]] - X_mean) / X_std\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "nans = X.isna().sum()\n",
    "print(nans[nans > 0])\n",
    "\n",
    "del nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train and Test sets\n",
    "\n",
    "We will split our data into training and testing sets so as to avoid overfitting and to retain a metric for evaluating our model on data that it hasn't seen during training.\n",
    "\n",
    "Depending on the amount of data, the sizes of the train and test sets are decided. Since we have around 5000 data elements we can split into 80-20 ratio or set a fixed number of examples for test set (for ex. 500).\n",
    "\n",
    "Here, we will retain 500 examples for testing and use the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First retrieve numpy arrays from the DataFrame and Series\n",
    "X_data = X.values\n",
    "Y_data = Y.values.reshape(Y.values.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size = 500, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "Y_train = Y_train.T\n",
    "X_test = X_test.T\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Neural Network -\n",
    "\n",
    "Set `layer_dims` and `layer_activations`\n",
    "\n",
    "Here, \n",
    "\n",
    "![NN design](nn_design.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [X_train.shape[0], 125, 64, 1]\n",
    "layer_activations = ['r', 'r', 's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[232, 125, 64, 1]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2pv4pjay) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1716<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e6f7fd29bc47adb5fba34b225183fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>E:\\Athang_Work\\AI-ML-Projects\\Neural Networks\\wandb\\run-20201227_173238-2pv4pjay\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>E:\\Athang_Work\\AI-ML-Projects\\Neural Networks\\wandb\\run-20201227_173238-2pv4pjay\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">solar-leaf-5</strong>: <a href=\"https://wandb.ai/xdevapps/dnn-impl/runs/2pv4pjay\" target=\"_blank\">https://wandb.ai/xdevapps/dnn-impl/runs/2pv4pjay</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2pv4pjay). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">usual-wind-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/xdevapps/dnn-impl\" target=\"_blank\">https://wandb.ai/xdevapps/dnn-impl</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/xdevapps/dnn-impl/runs/2t0ngdny\" target=\"_blank\">https://wandb.ai/xdevapps/dnn-impl/runs/2t0ngdny</a><br/>\n",
       "                Run data is saved locally in <code>E:\\Athang_Work\\AI-ML-Projects\\Neural Networks\\wandb\\run-20201227_173337-2t0ngdny</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.010148019997232945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-eec47015b9fb>:4: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = - (np.dot(Y, np.log(AL.T)) + np.dot(1 - Y, np.log(1 - AL.T))) / m\n",
      "<ipython-input-7-4d2d4548d011>:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dA = - np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1000: nan\n",
      "Cost after epoch 2000: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-94a5b505b150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_activations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'GD_Mom'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-103-a601fd861485>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, layer_dims, layer_activations, optimizer, learning_rate, mini_batch_size, beta1, beta2, epsilon, num_epochs, print_cost, enable_gradcheck)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Backward Propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_activations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Gradient Checking (if enabled)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-4d2d4548d011>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[1;34m(layer_dims, layer_activations, A, Y, caches, parameters)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_activations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propagation_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_cur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Store gradients for use in optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-48896ddb0eff>\u001b[0m in \u001b[0;36mbackward_propagation_layer\u001b[1;34m(dA, Z, A_prev, A, W, b, activation)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Calculate dW, db, dA_prev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_parameters, costs = model(X_train, Y_train, layer_dims, layer_activations, optimizer = 'GD', num_epochs = 10000, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs (per 100)')\n",
    "plt.title('Learning rate =' + str(0.0007))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7278026905829597\n",
      "Confusion Matrix: [[0, 1214], [3246, 0]]\n",
      "Precision: nan\n",
      "Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-84-19326eac3426>:12: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  print('Precision:', (tp / (tp + fp)))\n"
     ]
    }
   ],
   "source": [
    "train_predictions = predict(layer_dims, layer_activations, X_train, trained_parameters)\n",
    "\n",
    "_outdf1_ = pd.DataFrame(Y_train.T)\n",
    "_outdf2_ = pd.DataFrame(((train_predictions > 0.5) * 1).T)\n",
    "\n",
    "accuracy = accuracy_score(_outdf1_, _outdf2_)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(_outdf1_, _outdf2_).ravel()\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:', [[tp, fn], [tn, fp]])\n",
    "print('Precision:', (tp / (tp + fp)))\n",
    "print('Recall:', (tp / (tp + fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(Y_train.T).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688\n",
      "Confusion Matrix: [[0, 156], [344, 0]]\n",
      "Precision: nan\n",
      "Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-22b9d7210103>:12: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  print('Precision:', (tp / (tp + fp)))\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict(layer_dims, layer_activations, X_test, trained_parameters)\n",
    "\n",
    "_outdf1_ = pd.DataFrame(Y_test.T)\n",
    "_outdf2_ = pd.DataFrame(((test_predictions > 0.5) * 1).T)\n",
    "\n",
    "accuracy = accuracy_score(_outdf1_, _outdf2_)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(_outdf1_, _outdf2_).ravel()\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:', [[tp, fn], [tn, fp]])\n",
    "print('Precision:', (tp / (tp + fp)))\n",
    "print('Recall:', (tp / (tp + fn)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
